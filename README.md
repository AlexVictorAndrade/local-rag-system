# ğŸ§  Local RAG System â€” Productionâ€‘Style AI Retrieval Pipeline

A fully local **Retrieval Augmented Generation (RAG)** system built with modern AI infrastructure patterns.

This project demonstrates how to build a **privacyâ€‘first, productionâ€‘ready question answering system** that retrieves knowledge from custom documents and generates grounded answers using local LLMs.

No external APIs. No cloud dependency. Fully offline.

---

# âœ¨ Why this project exists

Most AI demos rely on cloud APIs and toy datasets.

This project focuses on what **real AI systems look like in production**:

âœ” Document ingestion pipeline
âœ” Semantic chunking
âœ” Embedding generation
âœ” Vector database retrieval
âœ” Contextâ€‘grounded LLM responses
âœ” API layer for consumption
âœ” Strict hallucination control
âœ” Fully local infrastructure

This is not a chatbot.
This is an **AI knowledge system**.

---

# ğŸ— System Architecture

```
Documents (PDF / TXT / MD)
        â†“
Text Chunking
        â†“
Embeddings (nomic-embed-text)
        â†“
Vector Database (ChromaDB)
        â†“
Similarity Retrieval
        â†“
Context Injection
        â†“
Local LLM (Mistral via Ollama)
        â†“
Grounded Answer
        â†“
FastAPI Endpoint
```

---

# ğŸ§© Tech Stack

### AI / ML

* Local LLM â†’ Mistral (Ollama)
* Embeddings â†’ nomic-embed-text
* Framework â†’ LangChain

### Retrieval

* Vector DB â†’ Chroma
* Semantic search â†’ cosine similarity

### Backend

* FastAPI
* Pydantic
* Uvicorn

### Infrastructure

* Python virtual environments
* Fully local inference
* No external API calls

---

# ğŸ”’ Privacy First Design

All processing happens locally:

âœ” Documents never leave your machine
âœ” Embeddings generated locally
âœ” LLM inference local
âœ” Vector DB local
âœ” No telemetry
âœ” No API keys

This architecture is ideal for:

* Enterprise internal knowledge bases
* Legal document search
* Financial reports
* Medical literature (offline environments)
* Secure environments

---

# ğŸ“ Project Structure

```
demo-rag-local/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py        # FastAPI application
â”‚   â”œâ”€â”€ ingest.py      # Document ingestion pipeline
â”‚   â””â”€â”€ rag.py         # Retrieval + generation logic
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/          # Source documents
â”‚
â”œâ”€â”€ chroma/            # Vector database (auto-generated)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸš€ Quick Start

## 1. Install Ollama

Mac:

```bash
brew install ollama
ollama serve
```

---

## 2. Download local models

```bash
ollama pull mistral
ollama pull nomic-embed-text
```

---

## 3. Create virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

---

## 4. Install dependencies

```bash
pip install -r requirements.txt
```

---

## 5. Add documents

Place files inside:

```
data/docs/
```

Supported formats:

* PDF
* TXT
* Markdown

---

## 6. Run ingestion pipeline

```bash
python app/ingest.py
```

This will:

âœ” Load documents
âœ” Split into semantic chunks
âœ” Generate embeddings
âœ” Store vectors in Chroma

---

## 7. Start API server

```bash
python -m uvicorn app.main:app --reload
```

Server:

```
http://127.0.0.1:8000
```

---

# ğŸ’¬ Query the system

## Swagger UI

```
http://127.0.0.1:8000/docs
```

POST `/question`

```json
{
  "question": "Your question here"
}
```

---

## Terminal request

```bash
curl -X POST "http://127.0.0.1:8000/question" \
-H "Content-Type: application/json" \
-d '{"question": "What is RAG?"}'
```

---

# ğŸ§  How hallucination is prevented

The LLM is strictly instructed to answer **only using retrieved context**.

If no relevant context exists â†’ the model must respond:

```
I cannot answer based on the provided context.
```

This ensures:

âœ” Grounded responses
âœ” Deterministic behavior
âœ” Enterprise reliability

---

# âš™ Core Components

## Ingestion Pipeline

* Document loading
* Recursive text splitting
* Embedding generation
* Vector persistence

## Retrieval Layer

* Topâ€‘K similarity search
* Context assembly
* Relevance filtering

## Generation Layer

* Context injection prompt
* Local LLM inference
* Controlled response policy

## API Layer

* Request validation
* RAG orchestration
* JSON response

---

# ğŸ“Š Performance Characteristics

Depends on hardware.

Typical laptop performance:

* Embedding generation â†’ fast (CPU ok)
* Retrieval â†’ < 100ms
* LLM response â†’ 1â€“4 seconds

---

# ğŸ¯ Real World Use Cases

* Internal company knowledge search
* Customer support automation
* Compliance document lookup
* Technical documentation assistant
* Research paper QA
* Offline AI environments

---

# ğŸ§ª Example Questions

âœ” "Summarize the policy document"
âœ” "What are the safety requirements?"
âœ” "Explain section 3.2"
âœ” "What deadlines are mentioned?"

---

# ğŸ”® Future Improvements

* Streaming responses
* Hybrid search (BM25 + vector)
* Re-ranking models
* Evaluation metrics (RAGAS)
* Web UI
* Docker container
* Multiâ€‘document collections
* Authentication layer
* Observability / tracing

---

# ğŸ§‘â€ğŸ’» Author

**Alex Victor de Andrade**
AI Engineer | Fullâ€‘Stack Developer | Applied AI Systems

Focus areas:

* Retrieval systems
* Generative AI infrastructure
* LLM applications
* Production AI architecture

---

# â­ If this project helped you

Consider giving it a star and sharing feedback.

---

# ğŸ“œ License

MIT License
